//! Chaperone: Server-side admin protocol for Holochain connection setup.
//!
//! Replaces the browser's 11-step admin WebSocket dance with a single
//! `POST /hc/connect` call. The browser generates signing keys locally,
//! sends the public key + cap secret, and doorway handles all admin
//! operations (cap grants, credential authorization, token issuance)
//! on the correct conductor.
//!
//! This eliminates:
//! - Two-WebSocket routing problem (admin + app must hit same conductor)
//! - Session affinity cache
//! - Admin protocol exposure in production
//! - Key format matching between browser and conductor

use base64::{engine::general_purpose::STANDARD as BASE64, Engine};
use bytes::Bytes;
use http_body_util::{BodyExt, Full};
use hyper::{Request, Response, StatusCode};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tracing::{debug, error, info, warn};

use crate::auth::{extract_token_from_header, JwtValidator, TokenInput};
use crate::conductor::{AdminClient, AgentProvisioner};
use crate::server::http::AppState;

/// Request body for `POST /hc/connect`.
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct ConnectRequest {
    /// Base64 Ed25519 public signing key (generated by browser)
    pub signing_key: String,
    /// Base64 capability secret (generated by browser)
    pub cap_secret: String,
}

/// Response body for `POST /hc/connect`.
#[derive(Debug, Serialize)]
#[serde(rename_all = "camelCase")]
pub struct ConnectResponse {
    /// Base64 Holochain app auth token for AppWebsocket
    pub app_token: String,
    /// App WebSocket port
    pub app_port: u16,
    /// Base64 agent public key
    pub agent_pub_key: String,
    /// Installed app ID
    pub installed_app_id: String,
    /// Conductor ID (also embedded in refreshed JWT)
    pub conductor_id: String,
    /// Cell IDs: { role_name: [dna_hash_b64, agent_key_b64] }
    pub cell_ids: HashMap<String, [String; 2]>,
    /// Refreshed JWT with conductor_id embedded
    pub token: String,
}

/// Handle `POST /hc/connect` — the Chaperone endpoint.
///
/// Orchestration flow:
/// 1. Extract JWT from Authorization header
/// 2. Resolve conductor (from claims.conductor_id, registry, or auto-provision)
/// 3. Get app info (cell IDs per role)
/// 4. Grant capabilities per cell (with source chain retry)
/// 5. Authorize signing credentials per cell (with retry)
/// 6. Get app interface port
/// 7. Issue app authentication token
/// 8. Re-issue JWT with conductor_id if not already present
/// 9. Return ConnectResponse
pub async fn handle_hc_connect(
    req: Request<hyper::body::Incoming>,
    state: Arc<AppState>,
) -> Response<Full<Bytes>> {
    // --- Step 1: Extract and validate JWT ---
    let auth_header = req
        .headers()
        .get(hyper::header::AUTHORIZATION)
        .and_then(|v| v.to_str().ok());

    let token_str = match extract_token_from_header(auth_header) {
        Some(t) => t.to_string(),
        None => {
            return json_error(StatusCode::UNAUTHORIZED, "Missing Authorization header");
        }
    };

    let jwt = if state.args.dev_mode {
        JwtValidator::new_dev()
    } else {
        match state.args.jwt_secret.as_ref() {
            Some(secret) => {
                match JwtValidator::new(secret.clone(), state.args.jwt_expiry_seconds) {
                    Ok(v) => v,
                    Err(_) => {
                        return json_error(StatusCode::INTERNAL_SERVER_ERROR, "JWT not configured")
                    }
                }
            }
            None => return json_error(StatusCode::INTERNAL_SERVER_ERROR, "JWT not configured"),
        }
    };

    let validation = jwt.verify_token(&token_str);
    let claims = match validation.claims {
        Some(c) if validation.valid => c,
        _ => return json_error(StatusCode::UNAUTHORIZED, "Invalid or expired token"),
    };

    // --- Read request body ---
    let body_bytes = match req.collect().await {
        Ok(collected) => collected.to_bytes(),
        Err(e) => {
            warn!("Failed to read /hc/connect body: {}", e);
            return json_error(StatusCode::BAD_REQUEST, "Failed to read request body");
        }
    };

    let body: ConnectRequest = match serde_json::from_slice(&body_bytes) {
        Ok(b) => b,
        Err(e) => {
            return json_error(StatusCode::BAD_REQUEST, &format!("Invalid JSON: {e}"));
        }
    };

    // Decode base64 inputs
    let signing_key = match BASE64.decode(&body.signing_key) {
        Ok(k) => k,
        Err(_) => return json_error(StatusCode::BAD_REQUEST, "Invalid base64 signing_key"),
    };
    let cap_secret = match BASE64.decode(&body.cap_secret) {
        Ok(s) => s,
        Err(_) => return json_error(StatusCode::BAD_REQUEST, "Invalid base64 cap_secret"),
    };

    // --- Step 2: Resolve conductor ---
    let registry = match &state.conductor_registry {
        Some(r) => Arc::clone(r),
        None => {
            return json_error(
                StatusCode::SERVICE_UNAVAILABLE,
                "No conductor registry available",
            );
        }
    };

    // Try conductor_id from JWT claims first
    let (conductor_id, admin_url, installed_app_id) = if let Some(ref cid) = claims.conductor_id {
        if let Some(info) = registry.get_conductor_info(cid) {
            let app_id = claims
                .installed_app_id
                .clone()
                .unwrap_or_else(|| state.args.installed_app_id.clone());
            (cid.clone(), info.admin_url, app_id)
        } else {
            return json_error(
                StatusCode::BAD_GATEWAY,
                &format!("Conductor '{}' not found in registry", cid),
            );
        }
    } else if let Some(entry) = registry.get_conductor_for_agent(&claims.agent_pub_key) {
        // Agent already registered in conductor registry
        let info = match registry.get_conductor_info(&entry.conductor_id) {
            Some(i) => i,
            None => {
                return json_error(StatusCode::BAD_GATEWAY, "Conductor info not found");
            }
        };
        (entry.conductor_id, info.admin_url, entry.app_id)
    } else {
        // Auto-provision (idempotent — handles logout→re-login)
        let provisioner = AgentProvisioner::new(Arc::clone(&registry))
            .with_app_id(state.args.installed_app_id.clone())
            .with_bundle_path(state.args.happ_bundle_path.clone());
        match provisioner.provision_agent(&claims.identifier).await {
            Ok(p) => {
                let info = match registry.get_conductor_info(&p.conductor_id) {
                    Some(i) => i,
                    None => {
                        return json_error(
                            StatusCode::BAD_GATEWAY,
                            "Provisioned conductor info not found",
                        );
                    }
                };

                // Also register the JWT's agent_pub_key so Path 2 works on future connects
                // (the provisioner registers the conductor-generated key, but the JWT
                // carries the auth-system key — both need to resolve to the same conductor)
                if claims.agent_pub_key != p.agent_pub_key {
                    let _ = registry
                        .register_agent(&claims.agent_pub_key, &p.conductor_id, &p.installed_app_id)
                        .await;
                }

                (p.conductor_id, info.admin_url, p.installed_app_id)
            }
            Err(e) => {
                error!("Chaperone auto-provision failed: {}", e);
                return sanitize_client_error(StatusCode::SERVICE_UNAVAILABLE, "Auto-provision");
            }
        }
    };

    info!(
        identifier = %claims.identifier,
        conductor = %conductor_id,
        app = %installed_app_id,
        "Chaperone: resolved conductor"
    );

    // --- Step 3: Create AdminClient and get app info (with cell readiness polling) ---
    let admin = AdminClient::new(admin_url);

    let app_info = {
        let max_polls = 20;
        let poll_interval = std::time::Duration::from_secs(1);
        let mut last_info = None;
        let mut app_found = false;

        for attempt in 1..=max_polls {
            match admin.get_app_info(&installed_app_id).await {
                Ok(info) => {
                    app_found = true;

                    if !info.cell_ids.is_empty() {
                        debug!(
                            app = %installed_app_id,
                            cells = info.cell_ids.len(),
                            polls = attempt,
                            "Cells ready"
                        );
                        last_info = Some(info);
                        break;
                    }

                    // App found but no cells yet — check if disabled
                    if let Some(ref status) = info.status {
                        if status.eq_ignore_ascii_case("disabled") {
                            warn!(
                                app = %installed_app_id,
                                "App is disabled — attempting re-enable"
                            );
                            if let Err(e) = admin.enable_app(&installed_app_id).await {
                                warn!(
                                    app = %installed_app_id,
                                    error = %e,
                                    "Failed to re-enable disabled app"
                                );
                            }
                        }
                    }

                    // Cells not ready yet — keep polling
                    if attempt < max_polls {
                        debug!(
                            app = %installed_app_id,
                            attempt,
                            status = ?info.status,
                            "Waiting for cells to initialize..."
                        );
                        tokio::time::sleep(poll_interval).await;
                    } else {
                        last_info = Some(info); // Last attempt — use whatever we got
                    }
                }
                Err(e) => {
                    if attempt == max_polls {
                        error!(
                            conductor = %conductor_id,
                            app = %installed_app_id,
                            error = %e,
                            attempts = max_polls,
                            "Chaperone: get_app_info failed after all attempts"
                        );
                        return sanitize_client_error(StatusCode::BAD_GATEWAY, "Get app info");
                    }
                    debug!(
                        app = %installed_app_id,
                        attempt,
                        error = %e,
                        "get_app_info not ready yet, retrying..."
                    );
                    tokio::time::sleep(poll_interval).await;
                }
            }
        }

        match last_info {
            Some(info) if !info.cell_ids.is_empty() => info,
            Some(info) => {
                warn!(
                    conductor = %conductor_id,
                    app = %installed_app_id,
                    status = ?info.status,
                    cell_count = info.cell_ids.len(),
                    "Chaperone: no cells after {}s polling",
                    max_polls
                );
                return json_error(
                    StatusCode::BAD_GATEWAY,
                    &format!(
                        "No cells found in app '{}' on conductor '{}' after {}s (status: {:?})",
                        installed_app_id, conductor_id, max_polls, info.status
                    ),
                );
            }
            None => {
                warn!(
                    conductor = %conductor_id,
                    app = %installed_app_id,
                    app_found,
                    "Chaperone: app info unavailable after polling"
                );
                return sanitize_client_error(StatusCode::BAD_GATEWAY, "Get app info");
            }
        }
    };

    // Track whether we've already attempted a re-enable for CellDisabled errors.
    // Only try once per connect — if it doesn't help, the cells need manual attention.
    let mut reenable_attempted = false;

    // --- Step 4: Grant capabilities per cell ---
    for (role_name, (ref dna_hash, ref agent_key)) in &app_info.cell_ids {
        let role = role_name.clone();
        let tag = format!("chaperone-{role}");
        let dna = dna_hash.clone();
        let agent = agent_key.clone();
        let sk = signing_key.clone();
        let cs = cap_secret.clone();

        let grant_result = AdminClient::with_source_chain_retry(
            || {
                let admin_inner = AdminClient::new(admin.admin_url().to_string());
                let dna = dna.clone();
                let agent = agent.clone();
                let cs = cs.clone();
                let sk = sk.clone();
                let tag = tag.clone();
                async move {
                    admin_inner
                        .grant_zome_call_capability((&dna, &agent), &cs, &sk, &tag)
                        .await
                }
            },
            &format!("cap grant for {role}"),
            3,
        )
        .await;

        if let Err(e) = grant_result {
            // CellMissing is non-fatal — skip that cell
            if e.contains("CellMissing") {
                warn!("Chaperone: skipping cap grant for '{}': CellMissing", role);
                continue;
            }

            // CellDisabled — attempt to re-enable the app and retry once
            if e.contains("CellDisabled") && !reenable_attempted {
                reenable_attempted = true;
                warn!(
                    app = %installed_app_id,
                    role = %role,
                    "Cell is disabled — attempting app re-enable and retry"
                );
                if let Err(enable_err) = admin.enable_app(&installed_app_id).await {
                    warn!(
                        app = %installed_app_id,
                        error = %enable_err,
                        "Failed to re-enable app after CellDisabled"
                    );
                } else {
                    // Wait briefly for cells to come back online
                    tokio::time::sleep(std::time::Duration::from_secs(2)).await;

                    // Retry the cap grant for this cell
                    let retry_admin = AdminClient::new(admin.admin_url().to_string());
                    match retry_admin
                        .grant_zome_call_capability((dna_hash, agent_key), &cs, &sk, &tag)
                        .await
                    {
                        Ok(()) => {
                            info!(role = %role, "Cap grant succeeded after re-enable");
                            continue;
                        }
                        Err(retry_err) => {
                            if retry_err.contains("CellMissing")
                                || retry_err.contains("CellDisabled")
                            {
                                warn!(
                                    "Chaperone: skipping cap grant for '{}' after re-enable: {}",
                                    role, retry_err
                                );
                                continue;
                            }
                            error!(
                                "Chaperone: cap grant still failed for '{}' after re-enable: {}",
                                role, retry_err
                            );
                            return sanitize_client_error(StatusCode::BAD_GATEWAY, "Cap grant");
                        }
                    }
                }
            }

            // CellDisabled after re-enable already attempted — skip
            if e.contains("CellDisabled") {
                warn!(
                    "Chaperone: skipping cap grant for '{}': CellDisabled (re-enable already attempted)",
                    role
                );
                continue;
            }

            error!("Chaperone: cap grant failed for '{}': {}", role, e);
            return sanitize_client_error(StatusCode::BAD_GATEWAY, "Cap grant");
        }
    }

    // --- Step 5: Authorize signing credentials per cell ---
    for (role_name, (ref dna_hash, ref agent_key)) in &app_info.cell_ids {
        let role = role_name.clone();
        let dna = dna_hash.clone();
        let agent = agent_key.clone();
        let sk = signing_key.clone();
        let cs = cap_secret.clone();

        let auth_result = AdminClient::with_source_chain_retry(
            || {
                let admin_inner = AdminClient::new(admin.admin_url().to_string());
                let dna = dna.clone();
                let agent = agent.clone();
                let sk = sk.clone();
                let cs = cs.clone();
                async move {
                    admin_inner
                        .authorize_signing_credentials((&dna, &agent), &sk, &cs)
                        .await
                }
            },
            &format!("authorize credentials for {role}"),
            3,
        )
        .await;

        if let Err(e) = auth_result {
            // CellMissing or CellDisabled — non-fatal, skip
            if e.contains("CellMissing") || e.contains("CellDisabled") {
                warn!("Chaperone: skipping authorize for '{}': {}", role, e);
                continue;
            }
            error!("Chaperone: authorize failed for '{}': {}", role, e);
            return sanitize_client_error(StatusCode::BAD_GATEWAY, "Authorize credentials");
        }
    }

    // --- Step 6: Get app interface port ---
    let app_port = match admin.list_app_interfaces().await {
        Ok(ports) if !ports.is_empty() => ports[0],
        Ok(_) => {
            warn!("Chaperone: no app interfaces found, using default 4445");
            4445
        }
        Err(e) => {
            warn!("Chaperone: list_app_interfaces failed: {}, using 4445", e);
            4445
        }
    };

    // --- Step 7: Issue app authentication token ---
    let app_token = match admin
        .issue_app_authentication_token(&installed_app_id, 3600)
        .await
    {
        Ok(token) => BASE64.encode(&token),
        Err(e) => {
            error!("Chaperone: issue_app_authentication_token failed: {}", e);
            return sanitize_client_error(StatusCode::BAD_GATEWAY, "Issue app token");
        }
    };

    // --- Step 8: Re-issue JWT with conductor_id ---
    let refreshed_token = if claims.conductor_id.is_none() || claims.installed_app_id.is_none() {
        let input = TokenInput {
            human_id: claims.human_id.clone(),
            agent_pub_key: claims.agent_pub_key.clone(),
            identifier: claims.identifier.clone(),
            permission_level: claims.permission_level,
            session_id: claims.session_id.clone(),
            doorway_id: state.args.doorway_id.clone(),
            doorway_url: state.args.doorway_url.clone(),
            conductor_id: Some(conductor_id.clone()),
            installed_app_id: Some(installed_app_id.clone()),
            is_steward: claims.is_steward,
            has_local_conductor: claims.has_local_conductor,
        };
        match jwt.generate_token(input) {
            Ok(t) => t,
            Err(e) => {
                warn!("Chaperone: failed to refresh JWT: {}", e);
                token_str.clone()
            }
        }
    } else {
        token_str.clone()
    };

    // --- Step 9: Build response ---
    let mut cell_ids_map = HashMap::new();
    for (role_name, (ref dna_hash, ref agent_key)) in &app_info.cell_ids {
        cell_ids_map.insert(
            role_name.clone(),
            [BASE64.encode(dna_hash), BASE64.encode(agent_key)],
        );
    }

    let response = ConnectResponse {
        app_token,
        app_port,
        agent_pub_key: BASE64.encode(&app_info.agent_pub_key),
        installed_app_id,
        conductor_id,
        cell_ids: cell_ids_map,
        token: refreshed_token,
    };

    info!(
        identifier = %claims.identifier,
        cells = response.cell_ids.len(),
        app_port = app_port,
        "Chaperone: connection established"
    );

    json_success(StatusCode::OK, &response)
}

// =============================================================================
// Helpers
// =============================================================================

/// Sanitize internal error details for client-facing responses.
/// Full errors are already logged server-side via `error!()` / `warn!()`.
fn sanitize_client_error(status: StatusCode, operation: &str) -> Response<Full<Bytes>> {
    let message = match status {
        StatusCode::BAD_GATEWAY => format!("{operation}: service temporarily unavailable"),
        StatusCode::SERVICE_UNAVAILABLE => "Service temporarily unavailable".to_string(),
        _ => format!("{operation} failed"),
    };
    json_error(status, &message)
}

fn json_error(status: StatusCode, message: &str) -> Response<Full<Bytes>> {
    let body = serde_json::json!({ "error": message });
    Response::builder()
        .status(status)
        .header("Content-Type", "application/json")
        .header("Access-Control-Allow-Origin", "*")
        .body(Full::new(Bytes::from(body.to_string())))
        .unwrap()
}

fn json_success<T: Serialize>(status: StatusCode, data: &T) -> Response<Full<Bytes>> {
    let body =
        serde_json::to_string(data).unwrap_or_else(|_| r#"{"error":"serialization"}"#.into());
    Response::builder()
        .status(status)
        .header("Content-Type", "application/json")
        .header("Access-Control-Allow-Origin", "*")
        .body(Full::new(Bytes::from(body)))
        .unwrap()
}
